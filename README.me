üß† Asistente RAG Local con IA (Ollama + Flutter + Python)

Una aplicaci√≥n de Chat con tus Documentos (RAG) que se ejecuta 100% localmente para m√°xima privacidad y velocidad.

üìã Descripci√≥n

Este proyecto implementa un sistema de Generaci√≥n Aumentada por Recuperaci√≥n (RAG) completamente local. Permite a los usuarios "chatear" con sus propios documentos (PDF, TXT, DOCX) sin que la informaci√≥n salga de su computadora.

A diferencia de las soluciones en la nube, esta arquitectura garantiza privacidad total y costo cero de operaci√≥n, utilizando modelos Open Source de √∫ltima generaci√≥n orquestados por Ollama.

üèóÔ∏è Arquitectura del Sistema

El sistema sigue una arquitectura cliente-servidor desacoplada para optimizar el rendimiento:

graph LR
    User[Usuario (Flutter App)] <-->API[Backend (FastAPI)]
    API <--> Logic[LangChain Orchestrator]
    Logic <--> VDB[(ChromaDB - Vector Store)]
    Logic <--> Embed[Nomic-Embed-Text (Embeddings)]
    Logic <--> LLM[Llama 3 (Inferencia)]


üöÄ Decisiones T√©cnicas Destacadas

Modelo de Embeddings Especializado (nomic-embed-text):

En lugar de usar un LLM pesado para leer documentos, se utiliza este modelo optimizado que es 20x m√°s r√°pido en la ingesti√≥n de datos y ofrece mejor recuperaci√≥n sem√°ntica.

Inferencia con Llama 3:

Utilizado estrictamente para la generaci√≥n de respuestas finales, aprovechando su capacidad de razonamiento.

Persistencia Vectorial (ChromaDB):

La base de datos vectorial se persiste en disco, evitando tener que re-procesar los documentos cada vez que se reinicia el servidor.

Backend As√≠ncrono (FastAPI):

Manejo eficiente de m√∫ltiples solicitudes y tiempos de espera largos t√≠picos de la IA.

üõ†Ô∏è Stack Tecnol√≥gico

IA & LLMs: Ollama, Llama 3, Nomic-Embed-Text.

Orquestaci√≥n: LangChain, LangChain-Core (LCEL).

Backend: Python, FastAPI, Uvicorn.

Base de Datos Vectorial: ChromaDB (Local).

Frontend: Flutter (Dart), Material Design 3.

Procesamiento de Archivos: PyPDF, Python-Docx.

üì¶ Instalaci√≥n y Configuraci√≥n

Prerrequisitos

Ollama: Debe estar instalado y ejecut√°ndose. Descargar Ollama.

Python 3.10+

Flutter SDK

Paso 1: Configurar Modelos de IA

El sistema descargar√° los modelos autom√°ticamente al iniciar el backend, pero puedes hacerlo manualmente para ahorrar tiempo:

ollama pull llama3
ollama pull nomic-embed-text


Paso 2: Iniciar el Backend

Navega a la carpeta backend y ejecuta el script de arranque autom√°tico (detecta OS y configura entorno virtual):

Windows: Doble click en run_backend.bat

Linux/Mac: Ejecutar ./run_backend.sh

El servidor iniciar√° en http://127.0.0.1:8000.

Paso 3: Iniciar la App (Frontend)

En una nueva terminal:

cd frontend
flutter pub get
flutter run


üéÆ Gu√≠a de Uso

Configuraci√≥n: Abre el panel de configuraci√≥n (icono de engranaje) en la app.

Ingesti√≥n: Pega la ruta local de tu carpeta de documentos (ej: C:\MisDocumentos\Tesis) y presiona "Cargar".

El sistema procesar√° los archivos a alta velocidad usando el modelo Nomic.

Chat: Cierra la configuraci√≥n y empieza a preguntar sobre tus archivos.

La IA citar√° las fuentes utilizadas para cada respuesta.

üîÆ Futuras Mejoras

Soporte para im√°genes y OCR.

Historial de conversaciones persistente (SQLite).

Selecci√≥n de modelos din√°mica desde la UI.

Modo "Resumen" para documentos largos.

Autor: Alberto Ghibaudo Otero
